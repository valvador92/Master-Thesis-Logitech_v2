@article{zollhofer_state_2018,
	title = {State of the {Art} on {3D} {Reconstruction} with {RGB}-{D} {Cameras}},
	volume = {37},
	issn = {01677055},
	url = {http://doi.wiley.com/10.1111/cgf.13386},
	doi = {10.1111/cgf.13386},
	abstract = {The advent of aﬀordable consumer grade RGB-D cameras has brought about a profound advancement of visual scene reconstruction methods. Both computer graphics and computer vision researchers spend signiﬁcant eﬀort to develop entirely new algorithms to capture comprehensive shape models of static and dynamic scenes with RGB-D cameras. This led to signiﬁcant advances of the state of the art along several dimensions. Some methods achieve very high reconstruction detail, despite limited sensor resolution. Others even achieve real-time performance, yet possibly at lower quality. New concepts were developed to capture scenes at larger spatial and temporal extent. Other recent algorithms ﬂank shape reconstruction with concurrent material and lighting estimation, even in general scenes and unconstrained conditions. In this state-of-the-art report, we analyze these recent developments in RGB-D scene reconstruction in detail and review essential related work. We explain, compare, and critically analyze the common underlying algorithmic concepts that enabled these recent advancements. Furthermore, we show how algorithms are designed to best exploit the beneﬁts of RGB-D data while suppressing their often non-trivial data distortions. In addition, this report identiﬁes and discusses important open research questions and suggests relevant directions for future work.},
	language = {en},
	number = {2},
	urldate = {2019-09-02},
	journal = {Computer Graphics Forum},
	author = {Zollhöfer, Michael and Stotko, Patrick and Görlitz, Andreas and Theobalt, Christian and Nießner, Matthias and Klein, Reinhard and Kolb, Andreas},
	month = may,
	year = {2018},
	pages = {625--652},
	file = {Zollhöfer et al. - 2018 - State of the Art on 3D Reconstruction with RGB-D C.pdf:C\:\\Users\\valmo\\Zotero\\storage\\6E2Z4ZUP\\Zollhöfer et al. - 2018 - State of the Art on 3D Reconstruction with RGB-D C.pdf:application/pdf}
}



@article{zollhofer_real-time_2014,
	title = {Real-time non-rigid reconstruction using an {RGB}-{D} camera},
	volume = {33},
	issn = {07300301},
	url = {http://dl.acm.org/citation.cfm?doid=2601097.2601165},
	doi = {10.1145/2601097.2601165},
	abstract = {We present a combined hardware and software solution for markerless reconstruction of non-rigidly deforming physical objects with arbitrary shape in real-time. Our system uses a single self-contained stereo camera unit built from off-the-shelf components and consumer graphics hardware to generate spatio-temporally coherent 3D models at 30 Hz. A new stereo matching algorithm estimates real-time RGB-D data. We start by scanning a smooth template model of the subject as they move rigidly. This geometric surface prior avoids strong scene assumptions, such as a kinematic human skeleton or a parametric shape model. Next, a novel GPU pipeline performs non-rigid registration of live RGB-D data to the smooth template using an extended non-linear as-rigid-as-possible (ARAP) framework. High-frequency details are fused onto the ﬁnal mesh using a linear deformation model. The system is an order of magnitude faster than state-of-the-art methods, while matching the quality and robustness of many ofﬂine algorithms. We show precise real-time reconstructions of diverse scenes, including: large deformations of users’ heads, hands, and upper bodies; ﬁne-scale wrinkles and folds of skin and clothing; and non-rigid interactions performed by users on ﬂexible objects such as toys. We demonstrate how acquired models can be used for many interactive scenarios, including re-texturing, online performance capture and preview, and real-time shape and motion re-targeting.},
	language = {en},
	number = {4},
	urldate = {2020-02-23},
	journal = {ACM Transactions on Graphics},
	author = {Zollhöfer, Michael and Theobalt, Christian and Stamminger, Marc and Nießner, Matthias and Izadi, Shahram and Rehmann, Christoph and Zach, Christopher and Fisher, Matthew and Wu, Chenglei and Fitzgibbon, Andrew and Loop, Charles},
	month = jul,
	year = {2014},
	pages = {1--12},
	file = {Zollhöfer et al. - 2014 - Real-time non-rigid reconstruction using an RGB-D .pdf:C\:\\Users\\valmo\\Zotero\\storage\\C8NC6K9Q\\Zollhöfer et al. - 2014 - Real-time non-rigid reconstruction using an RGB-D .pdf:application/pdf}
}




@article{dou_fusion4d:_2016,
	title = {{Fusion4D}: real-time performance capture of challenging scenes},
	volume = {35},
	issn = {07300301},
	shorttitle = {{Fusion4D}},
	url = {http://dl.acm.org/citation.cfm?doid=2897824.2925969},
	doi = {10.1145/2897824.2925969},
	abstract = {We contribute a new pipeline for live multi-view performance capture, generating temporally coherent high-quality reconstructions in real-time. Our algorithm supports both incremental reconstruction, improving the surface estimation over time, as well as parameterizing the nonrigid scene motion. Our approach is highly robust to both large frame-to-frame motion and topology changes, allowing us to reconstruct extremely challenging scenes. We demonstrate advantages over related real-time techniques that either deform an online generated template or continually fuse depth data nonrigidly into a single reference model. Finally, we show geometric reconstruction results on par with ofﬂine methods which require orders of magnitude more processing time and many more RGBD cameras.},
	language = {en},
	number = {4},
	urldate = {2019-09-03},
	journal = {ACM Transactions on Graphics},
	author = {Dou, Mingsong and Taylor, Jonathan and Kohli, Pushmeet and Tankovich, Vladimir and Izadi, Shahram and Khamis, Sameh and Degtyarev, Yury and Davidson, Philip and Fanello, Sean Ryan and Kowdle, Adarsh and Escolano, Sergio Orts and Rhemann, Christoph and Kim, David},
	month = jul,
	year = {2016},
	pages = {1--13},
	file = {Dou et al. - 2016 - Fusion4D real-time performance capture of challen.pdf:C\:\\Users\\valmo\\Zotero\\storage\\CEDUAU9T\\Dou et al. - 2016 - Fusion4D real-time performance capture of challen.pdf:application/pdf}
}


@article{du_montage4d:_2019,
	title = {{Montage4D}: {Seamless} {Fusion} and {Stylization} of {Multiview} {Video} {Textures}},
	volume = {8},
	language = {en},
	number = {1},
	author = {Du, Ruofei and Chuang, Ming and Chang, Wayne and Hoppe, Hugues and Varshney, Amitabh},
	year = {2019},
	pages = {34},
	file = {Du et al. - 2019 - Montage4D Seamless Fusion and Stylization of Mult.pdf:C\:\\Users\\valmo\\Zotero\\storage\\GX7LK3LY\\Du et al. - 2019 - Montage4D Seamless Fusion and Stylization of Mult.pdf:application/pdf}
}



@inproceedings{orts-escolano_holoportation_2016,
	address = {Tokyo, Japan},
	title = {Holoportation: {Virtual} {3D} {Teleportation} in {Real}-time},
	isbn = {978-1-4503-4189-9},
	shorttitle = {Holoportation},
	url = {http://dl.acm.org/citation.cfm?doid=2984511.2984517},
	doi = {10.1145/2984511.2984517},
	abstract = {We present an end-to-end system for augmented and virtual reality telepresence, called Holoportation. Our system demonstrates high-quality, real-time 3D reconstructions of an entire space, including people, furniture and objects, using a set of new depth cameras. These 3D models can also be transmitted in real-time to remote users. This allows users wearing virtual or augmented reality displays to see, hear and interact with remote participants in 3D, almost as if they were present in the same physical space. From an audio-visual perspective, communicating and interacting with remote users edges closer to face-to-face communication. This paper describes the Holoportation technical system in full, its key interactive capabilities, the application scenarios it enables, and an initial qualitative study of using this new communication medium.},
	language = {en},
	urldate = {2019-09-03},
	booktitle = {Proceedings of the 29th {Annual} {Symposium} on {User} {Interface} {Software} and {Technology}  - {UIST} '16},
	publisher = {ACM Press},
	author = {Orts-Escolano, Sergio and Dou, Mingsong and Tankovich, Vladimir and Loop, Charles and Cai, Qin and Chou, Philip A. and Mennicken, Sarah and Valentin, Julien and Pradeep, Vivek and Wang, Shenlong and Kang, Sing Bing and Rhemann, Christoph and Kohli, Pushmeet and Lutchyn, Yuliya and Keskin, Cem and Izadi, Shahram and Fanello, Sean and Chang, Wayne and Kowdle, Adarsh and Degtyarev, Yury and Kim, David and Davidson, Philip L. and Khamis, Sameh},
	year = {2016},
	pages = {741--754},
	file = {Orts-Escolano et al. - 2016 - Holoportation Virtual 3D Teleportation in Real-ti.pdf:C\:\\Users\\valmo\\Zotero\\storage\\W8B6YCS9\\Orts-Escolano et al. - 2016 - Holoportation Virtual 3D Teleportation in Real-ti.pdf:application/pdf}
}



@article{smolic_state---art_nodate,
	title = {{STATE}-{OF}-{THE}-{ART} {IN} {TELEPRESENCE}},
	language = {en},
	author = {Smolic, Aljosa and Knorr, Sebastian},
	year = {2018},
	pages = {47},
	file = {Smolic and Knorr - STATE-OF-THE-ART IN TELEPRESENCE.pdf:C\:\\Users\\valmo\\Zotero\\storage\\SH9SYIHY\\Smolic and Knorr - STATE-OF-THE-ART IN TELEPRESENCE.pdf:application/pdf}
}


@inproceedings{kowalski_livescan3d:_2015,
	address = {Lyon},
	title = {{Livescan3D}: {A} {Fast} and {Inexpensive} {3D} {Data} {Acquisition} {System} for {Multiple} {Kinect} v2 {Sensors}},
	isbn = {978-1-4673-8332-5},
	shorttitle = {{Livescan3D}},
	url = {https://ieeexplore.ieee.org/document/7335499/},
	doi = {10.1109/3DV.2015.43},
	abstract = {LiveScan3D is a free, open source system for live, 3D data acquisition using multiple Kinect v2 sensors. It allows the user to place any number of sensors in any physical conﬁguration and start gathering data at real time speed. The freedom of placing the sensors in any conﬁguration allows for many possible acquisition scenarios such as: capturing a single object from many viewpoints or creating 3D panoramas with multiple devices located close to each other. Thanks to the off-the-shelf Kinect v2 sensor the system is both accurate and inexpensive, opening 3D acquisition up to more recipients. In the paper we describe our system with the algorithms it is using and show its effectiveness in multiple scenarios including head shape reconstruction and 3D reconstruction of dynamic scenes.},
	language = {en},
	urldate = {2019-11-05},
	booktitle = {2015 {International} {Conference} on {3D} {Vision}},
	publisher = {IEEE},
	author = {Kowalski, Marek and Naruniec, Jacek and Daniluk, Michal},
	month = oct,
	year = {2015},
	pages = {318--325},
	file = {Kowalski et al. - 2015 - Livescan3D A Fast and Inexpensive 3D Data Acquisi.pdf:C\:\\Users\\valmo\\Zotero\\storage\\BNBYCRZ6\\Kowalski et al. - 2015 - Livescan3D A Fast and Inexpensive 3D Data Acquisi.pdf:application/pdf}
}


@article{pages_affordable_2018,
	title = {Affordable content creation for free-viewpoint video and {VR}/{AR} applications},
	volume = {53},
	issn = {10473203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1047320318300683},
	doi = {10.1016/j.jvcir.2018.03.012},
	abstract = {We present a scalable pipeline for Free-Viewpoint Video (FVV) content creation, considering also visualisation in Augmented Reality (AR) and Virtual Reality (VR). We support a range of scenarios where there may be a limited number of handheld consumer cameras, but also demonstrate how our method can be applied in professional multi-camera setups. Our novel pipeline extends many state-of-the-art techniques (such as structure-from-motion, shape-fromsilhouette and multi-view stereo) and incorporates bio-mechanical constraints through 3D skeletal information as well as eﬃcient camera pose estimation algorithms. We introduce multi-source shape-from-silhouette (MS-SfS) combined with fusion of diﬀerent geometry data as crucial components for accurate reconstruction in sparse camera settings. Our approach is highly ﬂexible and our results indicate suitability either for aﬀordable content creation for VR/AR or for interactive FVV visualisation where a user can choose an arbitrary viewpoint or sweep between known views using view synthesis.},
	language = {en},
	urldate = {2019-09-20},
	journal = {Journal of Visual Communication and Image Representation},
	author = {Pagés, R. and Amplianitis, K. and Monaghan, D. and Ondřej, J. and Smolić, A.},
	month = may,
	year = {2018},
	pages = {192--201},
	file = {Pagés et al. - 2018 - Affordable content creation for free-viewpoint vid.pdf:C\:\\Users\\valmo\\Zotero\\storage\\95VATRCV\\Pagés et al. - 2018 - Affordable content creation for free-viewpoint vid.pdf:application/pdf}
}


@misc{noauthor_epipolar_nodate,
	title = {Epipolar {Geometry} — {OpenCV}-{Python} {Tutorials} 1 documentation},
	url = {https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_calib3d/py_epipolar_geometry/py_epipolar_geometry.html},
	urldate = {2020-02-26},
	file = {Epipolar Geometry — OpenCV-Python Tutorials 1 documentation:C\:\\Users\\valmo\\Zotero\\storage\\6VXJWJJR\\py_epipolar_geometry.html:text/html}
}

@misc{noauthor_rotation_2020,
	title = {Rotation matrix},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Rotation_matrix&oldid=939053631},
	abstract = {In linear algebra, a rotation matrix is a matrix that is used to perform a rotation in Euclidean space. For example, using the convention below, the matrix

  
    
      
        R
        =
        
          
            [
            
              
                
                  cos
                  ⁡
                  θ
                
                
                  −
                  sin
                  ⁡
                  θ
                
              
              
                
                  sin
                  ⁡
                  θ
                
                
                  cos
                  ⁡
                  θ
                
              
            
            ]
          
        
      
    
    \{{\textbackslash}displaystyle R=\{{\textbackslash}begin\{bmatrix\}{\textbackslash}cos {\textbackslash}theta \&-{\textbackslash}sin {\textbackslash}theta {\textbackslash}{\textbackslash}{\textbackslash}sin {\textbackslash}theta \&{\textbackslash}cos {\textbackslash}theta {\textbackslash}{\textbackslash}{\textbackslash}end\{bmatrix\}\}\}
  rotates points in the xy-plane counterclockwise through an angle θ with respect to the x axis about the origin of a two-dimensional Cartesian coordinate system. To perform the rotation on a plane point with standard coordinates v = (x,y), it should be written as column vector, and multiplied by the matrix R:
    \{{\textbackslash}displaystyle R\{{\textbackslash}textbf \{v\}\}{\textbackslash} ={\textbackslash} \{{\textbackslash}begin\{bmatrix\}{\textbackslash}cos {\textbackslash}theta \&-{\textbackslash}sin {\textbackslash}theta {\textbackslash}{\textbackslash}{\textbackslash}sin {\textbackslash}theta \&{\textbackslash}cos {\textbackslash}theta {\textbackslash}end\{bmatrix\}\}{\textbackslash}cdot \{{\textbackslash}begin\{bmatrix\}x{\textbackslash}{\textbackslash}y{\textbackslash}end\{bmatrix\}\}{\textbackslash} ={\textbackslash} \{{\textbackslash}begin\{bmatrix\}x{\textbackslash}cos {\textbackslash}theta -y{\textbackslash}sin {\textbackslash}theta {\textbackslash}{\textbackslash}x{\textbackslash}sin {\textbackslash}theta +y{\textbackslash}cos {\textbackslash}theta {\textbackslash}end\{bmatrix\}\}.\}
  The examples in this article apply to active rotations of vectors counterclockwise in a right-handed coordinate system (y counterclockwise from x) by pre-multiplication (R on the left).  If any one of these is changed (such as rotating axes instead of vectors, a passive transformation), then the inverse of the example matrix should be used, which coincides with its transpose.
Since matrix multiplication has no effect on the zero vector (the coordinates of the origin), rotation matrices describe rotations about the origin. Rotation matrices provide an algebraic description of such rotations, and are used extensively for computations in geometry, physics, and computer graphics. In some literature, the term rotation is generalized to include improper rotations, characterized by orthogonal matrices with determinant −1 (instead of +1). These combine proper rotations with reflections (which invert orientation). In other cases, where reflections are not being considered, the label proper may be dropped. The latter convention is followed in this article.
Rotation matrices are square matrices, with real entries. More specifically, they can be characterized as orthogonal matrices with determinant 1; that is, a square matrix R is a rotation matrix if and only if RT = R−1 and det R = 1. The set of all orthogonal matrices of size  n with determinant +1 forms a group known as the special orthogonal group SO(n), one example of which is the rotation group SO(3). The set of all orthogonal matrices of size n with determinant +1 or −1 forms the (general) orthogonal group O(n).},
	language = {en},
	urldate = {2020-02-05},
	journal = {Wikipedia},
	month = feb,
	year = {2020},
	note = {Page Version ID: 939053631},
	file = {Snapshot:/home/valentin/snap/zotero-snap/10/Zotero/storage/JDQKFLMT/index.html:text/html}
}


@article{besl_method_1992,
	title = {A method for registration of 3-{D} shapes},
	volume = {14},
	issn = {0162-8828, 2160-9292},
	url = {http://ieeexplore.ieee.org/document/121791/},
	doi = {10.1109/34.121791},
	number = {2},
	urldate = {2020-02-06},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Besl, P.J. and McKay, Neil D.},
	month = feb,
	year = {1992},
	pages = {239--256},
	file = {Submitted Version:/home/valentin/snap/zotero-snap/10/Zotero/storage/VGKSAYP8/Besl and McKay - 1992 - A method for registration of 3-D shapes.pdf:application/pdf}
}


@article{chen_object_1992,
	title = {Object modelling by registration of multiple range images},
	volume = {10},
	issn = {02628856},
	url = {https://linkinghub.elsevier.com/retrieve/pii/026288569290066C},
	doi = {10.1016/0262-8856(92)90066-C},
	language = {en},
	number = {3},
	urldate = {2020-02-06},
	journal = {Image and Vision Computing},
	author = {Chen, Yang and Medioni, Gérard},
	month = apr,
	year = {1992},
	pages = {145--155}
}

@inproceedings{park_colored_2017,
	address = {Venice},
	title = {Colored {Point} {Cloud} {Registration} {Revisited}},
	isbn = {978-1-5386-1032-9},
	url = {http://ieeexplore.ieee.org/document/8237287/},
	doi = {10.1109/ICCV.2017.25},
	abstract = {We present an algorithm for aligning two colored point clouds. The key idea is to optimize a joint photometric and geometric objective that locks the alignment along both the normal direction and the tangent plane. We extend a photometric objective for aligning RGB-D images to point clouds, by locally parameterizing the point cloud with a virtual camera. Experiments demonstrate that our algorithm is more accurate and more robust than prior point cloud registration algorithms, including those that utilize color information. We use the presented algorithms to enhance a state-of-the-art scene reconstruction system. The precision of the resulting system is demonstrated on real-world scenes with accurate ground-truth models.},
	language = {en},
	urldate = {2020-02-06},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Park, Jaesik and Zhou, Qian-Yi and Koltun, Vladlen},
	month = oct,
	year = {2017},
	pages = {143--152},
	file = {Park et al. - 2017 - Colored Point Cloud Registration Revisited.pdf:/home/valentin/snap/zotero-snap/10/Zotero/storage/XTQ7EHNR/Park et al. - 2017 - Colored Point Cloud Registration Revisited.pdf:application/pdf}
}


@article{gelfand_robust_nodate,
	title = {Robust {Global} {Registration}},
	abstract = {We present an algorithm for the automatic alignment of two 3D shapes (data and model), without any assumptions about their initial positions. The algorithm computes for each surface point a descriptor based on local geometry that is robust to noise. A small number of feature points are automatically picked from the data shape according to the uniqueness of the descriptor value at the point. For each feature point on the data, we use the descriptor values of the model to ﬁnd potential corresponding points. We then develop a fast branch-and-bound algorithm based on distance matrix comparisons to select the optimal correspondence set and bring the two shapes into a coarse alignment. The result of our alignment algorithm is used as the initialization to ICP (iterative closest point) and its variants for ﬁne registration of the data to the model. Our algorithm can be used for matching shapes that overlap only over parts of their extent, for building models from partial range scans, as well as for simple symmetry detection, and for matching shapes undergoing articulated motion.},
	language = {en},
	author = {Gelfand, Natasha and Mitra, Niloy J and Guibas, Leonidas J and Pottmann, Helmut},
	pages = {10},
	file = {Gelfand et al. - Robust Global Registration.pdf:/home/valentin/snap/zotero-snap/10/Zotero/storage/VI6SNQZT/Gelfand et al. - Robust Global Registration.pdf:application/pdf}
}

@misc{noauthor_global_nodate,
	title = {Global registration — {Open3D} 0.9.0 documentation},
	url = {http://www.open3d.org/docs/release/tutorial/Advanced/global_registration.html#local-refinement},
	urldate = {2020-02-07},
	file = {Global registration — Open3D 0.9.0 documentation:/home/valentin/snap/zotero-snap/10/Zotero/storage/ZRAXTBBT/global_registration.html:text/html}
}


@inproceedings{rusu_fast_2009,
	address = {Kobe},
	title = {Fast {Point} {Feature} {Histograms} ({FPFH}) for {3D} registration},
	isbn = {978-1-4244-2788-8},
	url = {http://ieeexplore.ieee.org/document/5152473/},
	doi = {10.1109/ROBOT.2009.5152473},
	urldate = {2020-02-07},
	booktitle = {2009 {IEEE} {International} {Conference} on {Robotics} and {Automation}},
	publisher = {IEEE},
	author = {Rusu, Radu Bogdan and Blodow, Nico and Beetz, Michael},
	month = may,
	year = {2009},
	pages = {3212--3217}
}


@article{rathnayaka_efficient_2017,
	title = {An {Efficient} {Calibration} {Method} for a {Stereo} {Camera} {System} with {Heterogeneous} {Lenses} {Using} an {Embedded} {Checkerboard} {Pattern}},
	volume = {2017},
	issn = {1687-725X, 1687-7268},
	url = {https://www.hindawi.com/journals/js/2017/6742615/},
	doi = {10.1155/2017/6742615},
	abstract = {We present two simple approaches to calibrate a stereo camera setup with heterogeneous lenses: a wide-angle fish-eye lens and a narrow-angle lens in left and right sides, respectively. Instead of using a conventional black-white checkerboard pattern, we design an embedded checkerboard pattern by combining two differently colored patterns. In both approaches, we split the captured stereo images into RGB channels and extract R and inverted G channels from left and right camera images, respectively. In our first approach, we consider the checkerboard pattern as the world coordinate system and calculate left and right transformation matrices corresponding to it. We use these two transformation matrices to estimate the relative pose of the right camera by multiplying the inversed left transformation with the right. In the second approach, we calculate a planar homography transformation to identify common object points in left-right image pairs and treat them with the well-known Zhangs camera calibration method. We analyze the robustness of these two approaches by comparing reprojection errors and image rectification results. Experimental results show that the second method is more accurate than the first one.},
	language = {en},
	urldate = {2020-02-07},
	journal = {Journal of Sensors},
	author = {Rathnayaka, Pathum and Baek, Seung-Hae and Park, Soon-Yong},
	year = {2017},
	pages = {1--12},
	file = {Full Text:/home/valentin/snap/zotero-snap/10/Zotero/storage/HJGD74KF/Rathnayaka et al. - 2017 - An Efficient Calibration Method for a Stereo Camer.pdf:application/pdf}
}


@inproceedings{liu_stereo_2009,
	address = {Zhangjiajie, Hunan, China},
	title = {Stereo {Cameras} {Self}-{Calibration} {Based} on {SIFT}},
	isbn = {978-0-7695-3583-8},
	url = {http://ieeexplore.ieee.org/document/5202984/},
	doi = {10.1109/ICMTMA.2009.338},
	urldate = {2020-02-07},
	booktitle = {2009 {International} {Conference} on {Measuring} {Technology} and {Mechatronics} {Automation}},
	publisher = {IEEE},
	author = {Liu, Ran and Zhang, Hua and Liu, Manlu and Xia, Xianfeng and Hu, Tianlian},
	year = {2009},
	pages = {352--355}
}


@article{garrido-jurado_automatic_2014,
	title = {Automatic generation and detection of highly reliable fiducial markers under occlusion},
	volume = {47},
	issn = {00313203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320314000235},
	doi = {10.1016/j.patcog.2014.01.005},
	abstract = {This paper presents a ﬁducial marker system specially appropriated for camera pose estimation in applications such as augmented reality, robot localization, etc. Three main contributions are presented. First, we propose an algorithm for generating conﬁgurable marker dictionaries (in size and number of bits) following a criterion to maximize the inter-marker distance and the number of bit transitions. In the process, we derive the maximum theoretical inter-marker distance that dictionaries of square binary markers can have. Second, a method for automatically detecting the markers and correcting possible errors is proposed. Third, a solution to the occlusion problem in augmented reality applications is shown. To that aim, multiple markers are combined with an occlusion mask calculated by color segmentation. The experiments conducted show that our proposal obtains dictionaries with higher inter-marker distances and lower false negative rates than state-of-the-art systems, and provides an eﬀective solution to the occlusion problem.},
	language = {en},
	number = {6},
	urldate = {2020-02-26},
	journal = {Pattern Recognition},
	author = {Garrido-Jurado, S. and Muñoz-Salinas, R. and Madrid-Cuevas, F.J. and Marín-Jiménez, M.J.},
	month = jun,
	year = {2014},
	pages = {2280--2292},
	file = {Garrido-Jurado et al. - 2014 - Automatic generation and detection of highly relia.pdf:C\:\\Users\\valmo\\Zotero\\storage\\D8VYWXPQ\\Garrido-Jurado et al. - 2014 - Automatic generation and detection of highly relia.pdf:application/pdf}
}


@inproceedings{harris_combined_1988,
	address = {Manchester},
	title = {A {Combined} {Corner} and {Edge} {Detector}},
	url = {http://www.bmva.org/bmvc/1988/avc-88-023.html},
	doi = {10.5244/C.2.23},
	language = {en},
	urldate = {2020-02-26},
	booktitle = {Procedings of the {Alvey} {Vision} {Conference} 1988},
	publisher = {Alvey Vision Club},
	author = {Harris, C. and Stephens, M.},
	year = {1988},
	pages = {23.1--23.6},
	file = {Harris et Stephens - 1988 - A Combined Corner and Edge Detector.pdf:C\:\\Users\\valmo\\Zotero\\storage\\NPE6PTW4\\Harris et Stephens - 1988 - A Combined Corner and Edge Detector.pdf:application/pdf}
}


@misc{noauthor_opencv_nodate,
	title = {{OpenCV}},
	url = {https://opencv.org/},
	urldate = {2020-02-10},
	file = {OpenCV:/home/valentin/snap/zotero-snap/10/Zotero/storage/WT67Q9IN/opencv.org.html:text/html}
}


@article{Zhou2018,
	author    = {Qian-Yi Zhou and Jaesik Park and Vladlen Koltun},
	title     = {{Open3D}: {A} Modern Library for {3D} Data Processing},
	url = {http://www.open3d.org/},
	urldate = {2020-02-11},
	journal   = {arXiv:1801.09847},
	year      = {2018},
}

@article{pais19,
  title={3DRegNet: A Deep Neural Network for 3D Point Registration},
  author={G. Dias Pais and Pedro Miraldo and Srikumar Ramalingam and
            Jacinto C. Nascimento and Venu Madhav Govindu and Rama Chellappa},
  journal={arXiv:1904.01701},
  year={2019}
}

@inproceedings{xu_performance_2010,
	address = {San Francisco, CA, USA},
	title = {Performance evaluation of color correction approaches for automatic multi-view image and video stitching},
	isbn = {978-1-4244-6984-0},
	url = {http://ieeexplore.ieee.org/document/5540202/},
	doi = {10.1109/CVPR.2010.5540202},
	urldate = {2020-02-14},
	booktitle = {2010 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Xu, Wei and Mulligan, Jane},
	month = jun,
	year = {2010},
	pages = {263--270}
}



@article{reinhard_color_2001,
	title = {Color {Transfer} between {Images}},
	language = {en},
	journal = {IEEE Computer Graphics and Applications},
	author = {Reinhard, Erik and Ashikhmin, Michael and Gooch, Bruce and Shirley, Peter},
	year = {2001},
	pages = {8},
	file = {Reinhard et al. - 2001 - Color Transfer between Images.pdf:/home/valentin/snap/zotero-snap/10/Zotero/storage/H5M27PIH/Reinhard et al. - 2001 - Color Transfer between Images.pdf:application/pdf}
}


@inproceedings{dasari_reference_2016,
	address = {Victoria, BC, Canada},
	title = {Reference {Image} {Based} {Color} {Correction} for {Multi}-camera {Panoramic} {High} {Resolution} {Imaging}},
	isbn = {978-1-5090-2491-9},
	url = {http://ieeexplore.ieee.org/document/7801551/},
	doi = {10.1109/CRV.2016.15},
	abstract = {Maintaining color consistency across panoramic video captured by multi-camera array is a challenging problem. In an uncalibrated multi-camera array with automatic camera settings, each camera individually adjusts its parameters in accordance with the region of scene captured, independent of adjacent cameras. This leads to inconsistency in intensity and color in the stitched panoramic image. Selection of one of the images as reference for color correction may yield poor results because no individual camera image may represent the color palette of entire scene. We address these issues by capturing a separate low resolution reference color image with a ﬁeld of view that encompasses entire scene. The color statistics of the reference image are used to bring each camera array image into a uniform radiance and color palette. We then estimate optimal color correction parameters using a joint pairwise optimization that minimizes overall error in stitched panorama, thus achieving a fast and robust color correction scheme for multi-camera panoramic high resolution imaging.},
	language = {en},
	urldate = {2019-12-06},
	booktitle = {2016 13th {Conference} on {Computer} and {Robot} {Vision} ({CRV})},
	publisher = {IEEE},
	author = {Dasari, Radhakrishna and Zhang, Dong-Qing and Chen, Chang Wen},
	month = jun,
	year = {2016},
	pages = {410--415},
	file = {Dasari et al. - 2016 - Reference Image Based Color Correction for Multi-c.pdf:/home/valentin/snap/zotero-snap/10/Zotero/storage/LVXA8SGI/Dasari et al. - 2016 - Reference Image Based Color Correction for Multi-c.pdf:application/pdf}
}


@article{lowe_distinctive_2004,
	title = {Distinctive {Image} {Features} from {Scale}-{Invariant} {Keypoints}},
	volume = {60},
	issn = {0920-5691},
	url = {http://link.springer.com/10.1023/B:VISI.0000029664.99615.94},
	doi = {10.1023/B:VISI.0000029664.99615.94},
	abstract = {This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a a substantial range of afﬁne distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and ﬁnally performing veriﬁcation through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.},
	language = {en},
	number = {2},
	urldate = {2020-02-15},
	journal = {International Journal of Computer Vision},
	author = {Lowe, David G.},
	month = nov,
	year = {2004},
	pages = {91--110},
	file = {Lowe - 2004 - Distinctive Image Features from Scale-Invariant Ke.pdf:C\:\\Users\\valmo\\Zotero\\storage\\X27PW3MX\\Lowe - 2004 - Distinctive Image Features from Scale-Invariant Ke.pdf:application/pdf}
}


@inproceedings{gui_yun_tian_colour_2002,
	address = {London, UK},
	title = {Colour correction for panoramic imaging},
	isbn = {978-0-7695-1656-1},
	url = {http://ieeexplore.ieee.org/document/1028817/},
	doi = {10.1109/IV.2002.1028817},
	abstract = {This paper reports the problem of colour distortion in panoramic imaging. Particularly when image mosaic is used for panoramic imaging, the images are captured under different lighting conditions and viewpoints. The paper analyses several linear approaches for their colour transform and mapping. A new approach of colour histogram based colour correction is provided, which is robust to image capturing conditions such as viewpoints and scaling. The procedure for the colour correction is introduced and implemented. The conclusions are derived after experimental tests.},
	language = {en},
	urldate = {2020-02-17},
	booktitle = {Proceedings {Sixth} {International} {Conference} on {Information} {Visualisation}},
	publisher = {IEEE Comput. Soc},
	author = {{Gui Yun Tian} and Gledhill, D. and Taylor, D. and Clarke, D.},
	year = {2002},
	pages = {483--488},
	file = {Gui Yun Tian et al. - 2002 - Colour correction for panoramic imaging.pdf:/home/valentin/snap/zotero-snap/10/Zotero/storage/7TFAMZQR/Gui Yun Tian et al. - 2002 - Colour correction for panoramic imaging.pdf:application/pdf}
}


@inproceedings{yu-wing_tai_local_2005,
	address = {San Diego, CA, USA},
	title = {Local {Color} {Transfer} via {Probabilistic} {Segmentation} by {Expectation}-{Maximization}},
	volume = {1},
	isbn = {978-0-7695-2372-9},
	url = {http://ieeexplore.ieee.org/document/1467343/},
	doi = {10.1109/CVPR.2005.215},
	urldate = {2020-02-17},
	booktitle = {2005 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR}'05)},
	publisher = {IEEE},
	author = {{Yu-Wing Tai} and {Jiaya Jia} and {Chi-Keung Tang}},
	year = {2005},
	pages = {747--754},
	file = {Submitted Version:/home/valentin/snap/zotero-snap/10/Zotero/storage/MY2HBY8J/Yu-Wing Tai et al. - 2005 - Local Color Transfer via Probabilistic Segmentatio.pdf:application/pdf}
}


@inproceedings{eschbach_color_2015,
	address = {San Francisco, California, USA},
	title = {Color correction using {3D} multi-view geometry},
	url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2076867},
	doi = {10.1117/12.2076867},
	abstract = {Recently, many 3D contents production tools using multiview system has been introduced: e.g., depth estimation, 3D reconstruction and so forth. However, there is color mismatch problem in multiview system and it can cause big differences for the final result. In this paper we propose a color correction method using 3D multi-view geometry. The propose method finds correspondences between source and target viewpoint and calculates a translation matrix by using a polynomial regression technique. An experiment is performed in CIELab color space which is designed to approximate an human visual system and proposed method properly corrected the color compare to conventional methods. Moreover, we applied the proposed color correction method to 3D object reconstruction and we acquired a consistent 3D model in terms of color.},
	language = {en},
	urldate = {2019-11-05},
	author = {Shin, Dong-Won and Ho, Yo-Sung},
	editor = {Eschbach, Reiner and Marcu, Gabriel G. and Rizzi, Alessandro},
	month = feb,
	year = {2015},
	pages = {93950O},
	file = {Shin and Ho - 2015 - Color correction using 3D multi-view geometry.pdf:/home/valentin/snap/zotero-snap/10/Zotero/storage/KKWASI5R/Shin and Ho - 2015 - Color correction using 3D multi-view geometry.pdf:application/pdf}
}

@inbook{doi:10.1002/9781119975595.ch3,

publisher = {John Wiley & Sons, Ltd},
isbn = {9781119975595},
title = {Relations Between Colour Stimuli},
booktitle = {Measuring Colour},
chapter = {3},
pages = {41-72},
doi = {10.1002/9781119975595.ch3},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119975595.ch3},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781119975595.ch3},
year = {2011},
keywords = {chromaticity diagrams, CIELAB colour space, CIELUV colour space, colorimetric measures, tristimulus values, uniform chromaticity diagrams},
abstract = {Summary This chapter describes various colorimetric measures in terms of tristimulus values for the 1931 Observer; they can all also be evaluated using tristimulus values for the 1964 Observer, in which case they would all be distinguished by having a subscript 10. Chromaticity diagrams have many uses, but, as they show only proportions of tristimulus values, and not their actual magnitudes, they are only strictly applicable to colours that all have the same luminance and luminance factor. In general, colours differ in luminance and luminance factor, as well as in chromaticity, and some method of combining these variables is therefore required. To meet this need for luminance factor (but not for luminance), the CIE has recommended the use of one of CIELUV and CIELAB uniform colour spaces. CIELUV and CIELAB uniform colour spaces are described in the CIE Standards. Controlled Vocabulary Terms colorimetry; monochromators}
}

@inproceedings{ilie_ensuring_2005,
	address = {Beijing, China},
	title = {Ensuring color consistency across multiple cameras},
	isbn = {978-0-7695-2334-7},
	url = {http://ieeexplore.ieee.org/document/1544866/},
	doi = {10.1109/ICCV.2005.88},
	abstract = {Most multi-camera vision applications assume a single common color response for all cameras. However different cameras—even of the same type—can exhibit radically different color responses, and the differences can cause signiﬁcant errors in scene interpretation. To address this problem we have developed a robust system aimed at inter-camera color consistency. Our method consists of two phases: an iterative closed-loop calibration phase that searches for the per-camera hardware register settings that best balance linearity and dynamic range, followed by a reﬁnement phase that computes the per-camera parametric values for an additional software-based color mapping.},
	language = {en},
	urldate = {2020-02-18},
	booktitle = {Tenth {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV}'05) {Volume} 1},
	publisher = {IEEE},
	author = {Ilie, A. and Welch, G.},
	year = {2005},
	pages = {1268--1275 Vol. 2},
	file = {Ilie and Welch - 2005 - Ensuring color consistency across multiple cameras.pdf:/home/valentin/snap/zotero-snap/10/Zotero/storage/6K96QWPZ/Ilie and Welch - 2005 - Ensuring color consistency across multiple cameras.pdf:application/pdf}
}

@misc{noauthor_azure_nodate,
	title = {Azure {Kinect} {DK} – {Develop} {AI} {Models} {\textbar} {Microsoft} {Azure}},
	url = {https://azure.microsoft.com/en-us/services/kinect-dk/},
	abstract = {Developers: Build advanced computer vision and speech models with a 1-MP depth sensor,
12-MP RGB camera, and 7-mic audio array in a single device.},
	language = {en},
	urldate = {2020-02-26},
	note = {Library Catalog: azure.microsoft.com},
	file = {Snapshot:C\:\\Users\\valmo\\Zotero\\storage\\JYDYUBEL\\kinect-dk.html:text/html}
}


@misc{noauthor_dissecting_nodate,
	title = {Dissecting the {Camera} {Matrix}, {Part} 2: {The} {Extrinsic} {Matrix} ←},
	url = {http://ksimek.github.io/2012/08/22/extrinsic/},
	urldate = {2020-02-24},
	file = {Dissecting the Camera Matrix, Part 2\: The Extrinsic Matrix ←:/home/valentin/snap/zotero-snap/10/Zotero/storage/W94UQ4KK/extrinsic.html:text/html}
}


@misc{noauthor_glulookat_nodate,
	title = {{gluLookAt}},
	url = {https://www.khronos.org/registry/OpenGL-Refpages/gl2.1/xhtml/gluLookAt.xml},
	urldate = {2020-02-24},
	file = {gluLookAt:/home/valentin/snap/zotero-snap/10/Zotero/storage/TN9GUAUF/gluLookAt.html:text/html}
}


@article{kreiss_pifpaf_2019,
	title = {{PifPaf}: {Composite} {Fields} for {Human} {Pose} {Estimation}},
	shorttitle = {{PifPaf}},
	url = {http://arxiv.org/abs/1903.06593},
	abstract = {We propose a new bottom-up method for multiperson 2D human pose estimation that is particularly well suited for urban mobility such as self-driving cars and delivery robots. The new method, PifPaf, uses a Part Intensity Field (PIF) to localize body parts and a Part Association Field (PAF) to associate body parts with each other to form full human poses. Our method outperforms previous methods at low resolution and in crowded, cluttered and occluded scenes thanks to (i) our new composite ﬁeld PAF encoding ﬁne-grained information and (ii) the choice of Laplace loss for regressions which incorporates a notion of uncertainty. Our architecture is based on a fully convolutional, singleshot, box-free design. We perform on par with the existing state-of-the-art bottom-up method on the standard COCO keypoint task and produce state-of-the-art results on a modiﬁed COCO keypoint task for the transportation domain.},
	language = {en},
	urldate = {2020-02-24},
	journal = {arXiv:1903.06593 [cs]},
	author = {Kreiss, Sven and Bertoni, Lorenzo and Alahi, Alexandre},
	month = apr,
	year = {2019},
	note = {arXiv: 1903.06593},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Kreiss et al. - 2019 - PifPaf Composite Fields for Human Pose Estimation.pdf:/home/valentin/snap/zotero-snap/10/Zotero/storage/W3FZNS2V/Kreiss et al. - 2019 - PifPaf Composite Fields for Human Pose Estimation.pdf:application/pdf}
}



@article{umeyama_least-squares_1991,
	title = {Least-squares estimation of transformation parameters between two point patterns},
	volume = {13},
	issn = {01628828},
	url = {http://ieeexplore.ieee.org/document/88573/},
	doi = {10.1109/34.88573},
	number = {4},
	urldate = {2020-02-25},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Umeyama, S.},
	month = apr,
	year = {1991},
	pages = {376--380},
	file = {Version soumise:C\:\\Users\\valmo\\Zotero\\storage\\M56FAL8L\\Umeyama - 1991 - Least-squares estimation of transformation paramet.pdf:application/pdf}
}